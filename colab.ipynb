{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["PVLwrxpoEEEr","aDnVraTOEIsL","BhnIfK72DS63","l3mdHLSfD-jr"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PVLwrxpoEEEr","colab_type":"text"},"source":["## Install dependencies"]},{"cell_type":"code","metadata":{"id":"3Xct6QwTCun4","colab_type":"code","outputId":"59386e96-045d-4bb6-942c-2111fd7bed56","executionInfo":{"status":"ok","timestamp":1556980652059,"user_tz":240,"elapsed":51933,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":717}},"source":["!pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190504\n","\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly-gpu-2.0-preview\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/50/7369df696cfa8d93010a4acafd6a5b2a5ffc5a7ce6772581c6fef8ba1844/tf_nightly_gpu_2.0_preview-2.0.0.dev20190504-cp36-cp36m-manylinux1_x86_64.whl (346.5MB)\n","\u001b[K     |████████████████████████████████| 346.5MB 59kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n","Collecting wrapt>=1.11.1 (from tf-nightly-gpu-2.0-preview)\n","  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n","Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/72/7948e33a345f72ae84b598863b318153bba8c4b367496de5d89825f18cb1/tensorflow_estimator_2.0_preview-1.14.0.dev2019050400-py2.py3-none-any.whl (426kB)\n","\u001b[K     |████████████████████████████████| 430kB 44.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n","Collecting google-pasta>=0.1.2 (from tf-nightly-gpu-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n","\u001b[K     |████████████████████████████████| 61kB 27.2MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n","Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/51/39f71893b4e8ae1f242c2ac03c35b23d97bfb2ddb94c7597ba6fe022b0d5/tb_nightly-1.14.0a20190504-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 40.1MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n","Successfully built wrapt\n","\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n","Installing collected packages: wrapt, tensorflow-estimator-2.0-preview, google-pasta, tb-nightly, tf-nightly-gpu-2.0-preview\n","  Found existing installation: wrapt 1.10.11\n","    Uninstalling wrapt-1.10.11:\n","      Successfully uninstalled wrapt-1.10.11\n","Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190504 tensorflow-estimator-2.0-preview-1.14.0.dev2019050400 tf-nightly-gpu-2.0-preview-2.0.0.dev20190504 wrapt-1.11.1\n","2.0.0-dev20190504\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aDnVraTOEIsL","colab_type":"text"},"source":["## Integrate Google Drive"]},{"cell_type":"code","metadata":{"id":"RSdXWxcYZ8aK","colab_type":"code","outputId":"7790b3bd-06ed-4b56-c4c2-795dd81ba003","executionInfo":{"status":"ok","timestamp":1556980692997,"user_tz":240,"elapsed":31849,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8tu19Qn4Ed_p","colab_type":"text"},"source":["## Update Git Repository "]},{"cell_type":"code","metadata":{"id":"rdETFOFoCulD","colab_type":"code","outputId":"3e6856f2-585d-411f-8769-2958e0e0f477","executionInfo":{"status":"ok","timestamp":1556994591315,"user_tz":240,"elapsed":1612,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/humor_detection/\n","!git pull\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/humor_detection\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9NLLvcFsDNRJ","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"hUdKK461N0u0","colab_type":"code","outputId":"eb647d12-bf73-40d7-eb04-32ea0e73c941","executionInfo":{"status":"error","timestamp":1555779048962,"user_tz":240,"elapsed":8419,"user":{"displayName":"Gustavo Arango","photoUrl":"https://lh4.googleusercontent.com/-Uu1M-nqufbs/AAAAAAAAAAI/AAAAAAAAAeY/-TMCi_1ZbHg/s64/photo.jpg","userId":"06422916416085568647"}},"colab":{"base_uri":"https://localhost:8080/","height":1101}},"source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/humor_detection/\n","!python train.py --dataset_file ./data/dataset.tsv --checkpoint_path ./data/ --BATCH_SIZE 1000 --retrain"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/humor_detection\n","[2019-05-04 18:29:56,075] INFO - start\n","[2019-05-04 18:29:58,509] INFO - saving parameters to ./data/\n","[2019-05-04 18:29:58,512] INFO - loading dataset\n","2019-05-04 18:29:58.601093: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n","2019-05-04 18:29:58.645327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.645817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2019-05-04 18:29:58.645914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.646414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.646862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\n","2019-05-04 18:29:58.647252: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2019-05-04 18:29:58.783545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.784287: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e67dc0 executing computations on platform CUDA. Devices:\n","2019-05-04 18:29:58.784329: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2019-05-04 18:29:58.787400: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n","2019-05-04 18:29:58.787671: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e68a00 executing computations on platform Host. Devices:\n","2019-05-04 18:29:58.787702: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2019-05-04 18:29:58.787878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.788434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \n","name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n","pciBusID: 0000:00:04.0\n","2019-05-04 18:29:58.788506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.788857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.789176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\n","2019-05-04 18:29:58.789573: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n","2019-05-04 18:29:58.790513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2019-05-04 18:29:58.790540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 \n","2019-05-04 18:29:58.790553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N \n","2019-05-04 18:29:58.790820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.791215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2019-05-04 18:29:58.791606: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2019-05-04 18:29:58.791656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n","2019-05-04 18:29:58.977411: W tensorflow/core/common_runtime/executor.cc:2237] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n","\t [[{{node Placeholder/_0}}]]\n","2019-05-04 18:29:58.977965: W tensorflow/core/common_runtime/executor.cc:2237] [/device:CPU:0] Executor start aborting: Invalid argument: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n","\t [[{{node Placeholder/_0}}]]\n","[2019-05-04 18:29:58,988] WARNING - The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n","[2019-05-04 18:29:58,989] WARNING - The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n","[2019-05-04 18:29:58,990] WARNING - The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n","[2019-05-04 18:29:58,991] WARNING - The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n","[2019-05-04 18:29:58,991] WARNING - The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n","2019-05-04 18:30:02.049057: W tensorflow/core/framework/op_kernel.cc:1463] OP_REQUIRES failed at iterator_ops.cc:1034 : Invalid argument: All elements in a batch must have the same rank as the padded shape for component1: expected rank 1 but got element with rank 0\n","Traceback (most recent call last):\n","  File \"train.py\", line 259, in <module>\n","    train(args)\n","  File \"train.py\", line 84, in train\n","    example_entry = [i for i in train_dataset][0]\n","  File \"train.py\", line 84, in <listcomp>\n","    example_entry = [i for i in train_dataset][0]\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 586, in __next__\n","    return self.next()\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 623, in next\n","    return self._next_internal()\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 615, in _next_internal\n","    output_shapes=self._flat_output_shapes)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2120, in iterator_get_next_sync\n","    _six.raise_from(_core._status_to_exception(e.code, message), None)\n","  File \"<string>\", line 3, in raise_from\n","tensorflow.python.framework.errors_impl.InvalidArgumentError: All elements in a batch must have the same rank as the padded shape for component1: expected rank 1 but got element with rank 0 [Op:IteratorGetNextSync]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BhnIfK72DS63","colab_type":"text"},"source":["## Test the model"]},{"cell_type":"code","metadata":{"id":"4DgF6kv0DWCu","colab_type":"code","outputId":"902f3459-afd5-4545-ef1a-662af9468010","executionInfo":{"status":"ok","timestamp":1556332679934,"user_tz":240,"elapsed":9343,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/chatbot/transformer/\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import utensor.dataset as dt\n","from utensor.optimizer import CustomSchedule, loss_function\n","from utensor.model import Transformer\n","import time\n","from utensor.masking import create_masks\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","\n","\n","checkpoint_path=\"./data/banco/\"\n","d_model = 128\n","MAX_LENGTH=60\n","BUFFER_SIZE=20000\n","BATCH_SIZE=64\n","num_heads=8\n","num_layers=4\n","d_model=128\n","dff=512\n","dropout_rate=0.1\n","\n","\n","\n","def restore():\n","    \n","    # loading tokenizers for future predictions\n","    tokenizer_source = pickle.load(open(checkpoint_path + './tokenizer_source.pickle', 'rb'))\n","    tokenizer_target = pickle.load(open(checkpoint_path + './tokenizer_target.pickle', 'rb'))\n","\n","    input_vocab_size = tokenizer_source.vocab_size + 2\n","    target_vocab_size = tokenizer_target.vocab_size + 2\n","\n","    learning_rate = CustomSchedule(d_model)\n","    optimizer = tf.keras.optimizers.Adam(\n","        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n","    )\n","\n","    transformer = Transformer(\n","        num_layers,\n","        d_model,\n","        num_heads,\n","        dff,\n","        input_vocab_size,\n","        target_vocab_size,\n","        dropout_rate,\n","    )\n","\n","\n","    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","    # if a checkpoint exists, restore the latest checkpoint.\n","    if ckpt_manager.latest_checkpoint:\n","        ckpt.restore(ckpt_manager.latest_checkpoint)\n","        print(\"Latest checkpoint restored!!\")\n","    else:\n","        print(\"Initializing from scratch.\")\n","        \n","    return transformer, tokenizer_source, tokenizer_target\n","\n","\n","           \n","    \n","def evaluate(inp_sentence):\n","    start_token = [tokenizer_source.vocab_size]\n","    end_token = [tokenizer_source.vocab_size + 1]\n","\n","    # inp sentence is portuguese, hence adding the start and end token\n","    inp_sentence = start_token + tokenizer_source.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","    # as the target is english, the first word to the transformer should be the\n","    # english start token.\n","    decoder_input = [tokenizer_target.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","\n","    for i in range(MAX_LENGTH):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","            encoder_input, output)\n","\n","        # predictions.shape == (batch_size, seq_len, vocab_size)\n","        predictions, attention_weights = transformer(encoder_input, \n","                                                     output,\n","                                                     False,\n","                                                     enc_padding_mask,\n","                                                     combined_mask,\n","                                                     dec_padding_mask)\n","\n","        # select the last word from the seq_len dimension\n","        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        # return the result if the predicted_id is equal to the end token\n","        if tf.equal(predicted_id, tokenizer_target.vocab_size+1):\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        # concatentate the predicted_id to the output which is given to the decoder\n","        # as its input.\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","\n","def translate(sentence):\n","    result, attention_weights = evaluate(sentence)\n","\n","    predicted_sentence = tokenizer_target.decode([i for i in result \n","                                            if i < tokenizer_target.vocab_size])  \n","\n","    print('Pregunta: {}'.format(sentence))\n","    print('Respuesta UmyBot: {}'.format(predicted_sentence))\n","\n","\n","\n","transformer, tokenizer_source, tokenizer_target = restore()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer\n","Latest checkpoint restored!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vDO-wQ-E56qb","colab_type":"code","outputId":"b5d9eec9-22ed-44b6-9e99-a22559aa291f","executionInfo":{"status":"ok","timestamp":1556332690782,"user_tz":240,"elapsed":7998,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["translate('banco_falabella dicen que no están operativo')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pregunta: banco_falabella dicen que no están operativo\n","Respuesta UmyBot: hola @fefith, hemos realizado pruebas de transferencia y no presentamos inconvenientes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QLpF3esoFT31","colab_type":"code","outputId":"dbac0380-e193-41c6-aa6d-83b2fe582987","executionInfo":{"status":"ok","timestamp":1556254883833,"user_tz":240,"elapsed":2903,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["translate('@bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pregunta: @bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.\n","Respuesta UmyBot: ¡hola! cuéntanos por favor si haces referencia a un poco más de tu comentario, para poder ayudarte. saludos. ana\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l3mdHLSfD-jr","colab_type":"text"},"source":["## Predict examples"]},{"cell_type":"code","metadata":{"id":"a2ebWi0cUzow","colab_type":"code","outputId":"c3e562e0-d5fd-4f58-cfa7-f631383ca55c","executionInfo":{"status":"ok","timestamp":1556252317547,"user_tz":240,"elapsed":1220,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/chatbot/transformer/\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import utensor.dataset as dt\n","from utensor.optimizer import CustomSchedule, loss_function\n","from utensor.model import Transformer\n","import time\n","from utensor.masking import create_masks\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","checkpoint_path=\"./data/banco/\"\n","d_model = 128\n","MAX_LENGTH=40\n","BUFFER_SIZE=20000\n","BATCH_SIZE=64\n","num_heads=8\n","num_layers=4\n","dff=512\n","dropout_rate=0.1\n","\n","def restore():\n","\n","    # loading tokenizers for future predictions\n","    tokenizer_source = pickle.load(open(checkpoint_path+'/tokenizer_source.pickle', 'rb'))\n","    tokenizer_target = pickle.load(open(checkpoint_path+'/tokenizer_target.pickle', 'rb'))\n","\n","    input_vocab_size = tokenizer_source.vocab_size + 2\n","    target_vocab_size = tokenizer_target.vocab_size + 2\n","\n","    learning_rate = CustomSchedule(d_model)\n","    optimizer = tf.keras.optimizers.Adam(\n","        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n","    )\n","\n","    transformer = Transformer(\n","        num_layers,\n","        d_model,\n","        num_heads,\n","        dff,\n","        input_vocab_size,\n","        target_vocab_size,\n","        dropout_rate,\n","    )\n","\n","    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","    # if a checkpoint exists, restore the latest checkpoint.\n","    if ckpt_manager.latest_checkpoint:\n","        ckpt.restore(ckpt_manager.latest_checkpoint)\n","        print(\"Latest checkpoint restored!!\")\n","    else:\n","        print(\"Initializing from scratch.\")\n","\n","    return transformer, tokenizer_source, tokenizer_target\n","        \n","\n","        \n","def evaluate(inp_sentence):\n","    start_token = [tokenizer_source.vocab_size]\n","    end_token = [tokenizer_source.vocab_size + 1]\n","    \n","    \n","    # inp sentence is portuguese, hence adding the start and end token\n","    inp_sentence = start_token + tokenizer_source.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","    # as the target is english, the first word to the transformer should be the\n","    # english start token.\n","    decoder_input = [tokenizer_target.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","\n","    for i in range(40):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","\n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","\n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # return the result if the predicted_id is equal to the end token\n","    if tf.equal(predicted_id, tokenizer_target.vocab_size+1):\n","        return tf.squeeze(output, axis=0), attention_weights\n","\n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","\n","\n","def translate(sentence, plot=''):\n","    \n","    result, attention_weights = evaluate(sentence)\n","\n","    predicted_sentence = tokenizer_target.decode([i for i in result \n","                                            if i < tokenizer_target.vocab_size])  \n","\n","    print('Pregunta: {}'.format(sentence))\n","    print('Respuesta UmyBot: {}'.format(predicted_sentence))\n","\n","    #   if plot:\n","    #     plot_attention_weights(attention_weights, sentence, result, plot)\n","\n","    \n","    \n","transformer, tokenizer_source, tokenizer_target = restore()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer\n","Latest checkpoint restored!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0xqAvczzvMok","colab_type":"code","outputId":"96cc8f2c-4309-44af-950c-768042801af8","executionInfo":{"status":"ok","timestamp":1556252323817,"user_tz":240,"elapsed":1518,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["sentence = '@bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.'\n","translate(sentence, plot='')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pregunta: @bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.\n","Respuesta UmyBot: ¡\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"97w-DBMBn3JE","colab_type":"code","colab":{}},"source":["import pandas as pd\n","data = pd.read_csv('./data/banco/bancobot.tsv.test', sep='\\t', names=['source', 'target'])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DOJWQxVCuLm","colab_type":"code","colab":{}},"source":["\n","for ix,i in data.iterrows():\n","    translate(\n","        i['source']\n","    )\n","    print(\"Respuesta Humano: {}\".format(i['target']))\n","    print('\\n\\n')\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISiMWAVqft2e","colab_type":"code","colab":{}},"source":["def plot_attention_weights(attention, sentence, result, layer, tokenizer_source, tokenizer_target):\n","  fig = plt.figure(figsize=(30, 38))\n","  \n","  sentence = tokenizer_source.encode(sentence)\n","  \n","  attention = tf.squeeze(attention[layer], axis=0)\n","  \n","  for head in range(attention.shape[0]):\n","    ax = fig.add_subplot(8, 1, head+1)\n","    \n","    # plot the attention weights\n","    ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","    fontdict = {'fontsize': 10}\n","    \n","    ax.set_xticks(range(len(sentence)+2))\n","    ax.set_yticks(range(len(result)))\n","    \n","    ax.set_ylim(len(result)-1.5, -0.5)\n","        \n","    ax.set_xticklabels(\n","        ['<start>']+[tokenizer_source.decode([i]) for i in sentence]+['<end>'], \n","        fontdict=fontdict, rotation=90)\n","    \n","    ax.set_yticklabels([tokenizer_target.decode([i]) for i in result \n","                        if i < tokenizer_target.vocab_size], \n","                       fontdict=fontdict)\n","    \n","    ax.set_xlabel('Head {}'.format(head+1))\n","  \n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]}]}